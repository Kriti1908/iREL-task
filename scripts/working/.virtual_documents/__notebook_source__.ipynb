get_ipython().getoutput("pip install -q \")
    transformers \
    accelerate \
    peft \
    bitsandbytes \
    datasets \
    sentence-transformers \
    faiss-cpu \
    pymupdf


# !pip install faiss-cpu


import torch
import faiss
import json
import numpy as np
import fitz
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model


MODEL_NAME = "Qwen/Qwen3-4B"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto"
)


lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


dataset = load_dataset(
    "json",
    data_files="/kaggle/input/lora-train-json/lora_train.jsonl"
)

def tokenize(example):
    messages = [
        {"role": "user", "content": example["instruction"]},
        {"role": "assistant", "content": example["response"]}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )

    out = tokenizer(
        text,
        truncation=True,
        max_length=512
    )
    out["labels"] = out["input_ids"].copy()
    return out

dataset = dataset.map(tokenize, remove_columns=dataset["train"].column_names)


args = TrainingArguments(
    output_dir="lora_out",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=2,
    logging_steps=10,
    save_strategy="epoch",
    optim="paged_adamw_8bit",
    fp16=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset["train"]
)

trainer.train()


model.save_pretrained("qwen3-4b-course")
tokenizer.save_pretrained("qwen3-4b-course")


index = faiss.read_index("/kaggle/input/vectorstore/index.faiss")
texts = np.load("/kaggle/input/vectorstore/texts.npy", allow_pickle=True)

embedder = SentenceTransformer("intfloat/e5-base-v2")


def youtube_search(q):
    return f"https://www.youtube.com/results?search_query={q.replace(' ', '+')}"

def arxiv_search(q):
    return f"https://arxiv.org/search/?query={q.replace(' ', '+')}&searchtype=all"


def retrieve_context(question, k=4):
    emb = embedder.encode([question]).astype("float32")
    _, idx = index.search(emb, k)
    return "\n\n".join(texts[i] for i in idx[0])


def generate_answer(question):
    context = retrieve_context(question)

    messages = [
        {
            "role": "system",
            "content": "You are an IIIT course assistant."
        },
        {
            "role": "user",
            "content": f"""
Context:
{context}

Question:
{question}

Answer clearly, completely, and in a structured manner.
"""
        }
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    output = model.generate(
        **inputs,
        max_new_tokens=2048,
        temperature=0.6,
        top_p=0.95
    )

    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    # ‚úÖ ADD THESE
    yt_links = youtube_search(question)
    arxiv_links = arxiv_search(question)

    # Safety: ensure lists
    if isinstance(yt_links, str):
        yt_links = [yt_links]
    if isinstance(arxiv_links, str):
        arxiv_links = [arxiv_links]

    return {
        "answer": answer,
        "youtube": yt_links,
        "papers": arxiv_links
    }





# üîç Trial test cell ‚Äî verify RAG + LoRA model works end-to-end
test_question = "What is a random variable? Explain with examples."

print("=" * 70)
print(" IIIT Course Assistant ‚Äì Trial RAG Test")
print("=" * 70)

print("\nüßë‚Äçüéì Question:")
print(test_question)

print("\nüîç Thinking...\n")

result = generate_answer(test_question)

print("üìò Answer:")
print("-" * 70)
print(result["answer"].strip())
print("-" * 70)

if result.get("youtube"):
    print("\nüì∫ Suggested YouTube Videos:")
    for link in result["youtube"]:
        print("  ‚Ä¢", link)

if result.get("papers"):
    print("\nüìÑ Relevant Research Papers:")
    for paper in result["papers"]:
        print("  ‚Ä¢", paper)

print("\n" + "=" * 70)
print("‚úÖ Trial test completed successfully")





# doc = fitz.open("/kaggle/input/endsem/endsem.pdf")
# questions = []

# for page in doc:
#     for line in page.get_text().split("\n"):
#         if line.lower().startswith(("q", "question")):
#             questions.append(line)

# results = {}
# for i, q in enumerate(questions):
#     results[f"Q{i+1}"] = generate_answer(q)

# with open("endsem_results.json", "w") as f:
#     json.dump(results, f, indent=2)
import json
import re

PDF_PATH = "/kaggle/input/endsem/endsem_solutions.pdf" 

def extract_questions_from_solutions(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text()
    
    # regex to find patterns like "Q1:", "Q2:", "(a)", "(b)" etc.
    # This splits the text based on "Q" followed by a number and a colon
    question_blocks = re.split(r'(Q\d+[:])', full_text)
    
    extracted_questions = []
    # The first element is usually header text, so we skip it
    for i in range(1, len(question_blocks), 2):
        q_marker = question_blocks[i]      # e.g., "Q1:"
        q_content = question_blocks[i+1]    # The text following it
        
        # We stop the question text before the answer "A:" begins
        actual_question = q_content.split('A:')[0].strip()
        extracted_questions.append(f"{q_marker} {actual_question}")
        
    return extracted_questions

# 1. Extract the questions
print("Reading questions from PDF...")
questions_to_test = extract_questions_from_solutions(PDF_PATH)

# 2. Run inference on each extracted question
results = {}
print(f"Found {len(questions_to_test)} questions. Starting inference...\n")

for i, q_text in enumerate(questions_to_test):
    print(f"Testing {q_text[:50]}...")
    try:
        # Using the generate_answer function from your notebook
        ans_data = generate_answer(q_text)
        results[f"Question_{i+1}"] = {
            "input_question": q_text,
            "agent_answer": ans_data["answer"],
            "suggested_resources": {
                "youtube": ans_data["youtube"],
                "papers": ans_data["papers"]
            }
        }
    except Exception as e:
        print(f"Error processing question {i+1}: {e}")

# 3. Save the agent's performance to a JSON file
output_file = "agent_test_results.json"
with open(output_file, "w") as f:
    json.dump(results, f, indent=4)

print(f"\n‚úÖ Testing complete. Results saved to {output_file}")
